{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lmtw_pSbGjCY",
    "outputId": "157029d4-c2f5-4cfd-869d-f4aa97eae851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5fYVvVnPkzJk"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_I5X_GW80eo",
    "outputId": "00bfe05c-1480-4df0-a8fa-a76c613123ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download RAF-DB dataset (Goole Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0OqFiGiKzIg",
    "outputId": "b7adb430-6fda-4b2b-fe9b-3db3c0c56afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-27 21:43:48--  https://github.com/MegaloPat/DNN/blob/main/DNN/aligned.zip\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘aligned.zip’\n",
      "\n",
      "aligned.zip             [ <=>                ] 132.50K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-01-27 21:43:48 (12.1 MB/s) - ‘aligned.zip’ saved [135684]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget \"https://github.com/MegaloPat/DNN/blob/main/DNN/aligned.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget \"https://github.com/MegaloPat/DNN/blob/main/DNN/landmark.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained VGG_Face weights (Goole Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xaaP6dBLZM8",
    "outputId": "7b14f48e-05a3-4982-d89b-4d86e92e4218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-27 21:42:37--  https://www.robots.ox.ac.uk/~albanie/models/pytorch-mcn/vgg_face_dag.pth\n",
      "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
      "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 580015466 (553M)\n",
      "Saving to: ‘vgg_face_dag.pth’\n",
      "\n",
      "vgg_face_dag.pth    100%[===================>] 553.15M  14.3MB/s    in 41s     \n",
      "\n",
      "2023-01-27 21:43:19 (13.5 MB/s) - ‘vgg_face_dag.pth’ saved [580015466/580015466]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://www.robots.ox.ac.uk/~albanie/models/pytorch-mcn/vgg_face_dag.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK-qn9hFeHqA"
   },
   "source": [
    "# I - Vanilla classification with pretrained VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try to get baseline results with VGG pretrained on VGG_Face without any changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFWWOXXueWol"
   },
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAljxGKMeQ0g"
   },
   "source": [
    "### Unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5V-l3Mf6TTQq"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"aligned.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j3hUoGmxTTQr"
   },
   "outputs": [],
   "source": [
    "!mkdir aligned/train\n",
    "!mkdir aligned/test\n",
    "!mv aligned/aligned/train_* aligned/train\n",
    "!mv aligned/aligned/test_* aligned/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9KfQpHqebCw"
   },
   "source": [
    "### Prepare csv labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_gf_GRWUTTQs"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"list_patition_label.txt\",\"r\") as file :\n",
    "    train_csv = open(\"train_list_label.csv\",\"w\",newline=\"\")\n",
    "    test_csv = open(\"test_list_label.csv\",\"w\",newline=\"\")\n",
    "\n",
    "    train_writer = csv.writer(train_csv)\n",
    "    train_writer.writerow([\"Filename\", \"Label\"])\n",
    "    \n",
    "    test_writer = csv.writer(test_csv)\n",
    "    test_writer.writerow([\"Filename\", \"Label\"])\n",
    "    \n",
    "    \n",
    "    for line in file:\n",
    "        filename, label = line.strip().split(\" \")\n",
    "        idx = filename.index(\".jpg\")\n",
    "        filename = filename[:idx] + \"_aligned\" + filename[idx:]\n",
    "        label = str(int(label) - 1)\n",
    "        \n",
    "        if \"train\" in filename :\n",
    "            train_writer.writerow([filename, label])\n",
    "        else :\n",
    "            test_writer.writerow([filename, label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTAvN8J3egfk"
   },
   "source": [
    "### Preprocessing transform\n",
    "Preprocessing is the same as the experimental protocol of the original paper. This includes :\n",
    "* Resize to 224x224\n",
    "* Random rotation of -10° +10°\n",
    "* Random horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9MF1-iFsTTQt"
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.float()),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaC7JedKenG_"
   },
   "source": [
    "### Create dataloaders\n",
    "Validation set will be sampled with a stratified split of test set, as in the paper, with ratio 50/50.\n",
    "\n",
    "Overall the proportions for the train/test/validation datasets are 80/10/10 %\n",
    "\n",
    "Finally, we will use a batch size of 16 as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RhF0_COyTTQv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFB9WkgxTTQx",
    "outputId": "eada08fa-23aa-46c5-d269-20d36cdb0145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nb batches in train: 758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_data = CustomImageDataset(\"train_list_label.csv\",\"./aligned/train\", transform=trans)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4)\n",
    "print(f\"\\nNb batches in train: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "I2paf1E6TTQy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_data = CustomImageDataset(\"test_list_label.csv\",\"./aligned/test\", transform=trans)\n",
    "\n",
    "test_indices, val_indices = train_test_split(list(range(len(test_data.img_labels.Label))), test_size=0.5, stratify=test_data.img_labels.Label)\n",
    "\n",
    "val_data = torch.utils.data.Subset(test_data, val_indices)\n",
    "test_data = torch.utils.data.Subset(test_data, test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0vmJvM_c96n",
    "outputId": "660aef5a-f8d8-49cf-d0a4-8a44ea502e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nb batches in test: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=True, num_workers=4)\n",
    "print(f\"\\nNb batches in test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Cztj6-wc_iE",
    "outputId": "8df05571-12cf-4acc-99d3-114128ad9ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nb batches in val: 92\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=True, num_workers=4)\n",
    "print(f\"\\nNb batches in val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpPiJoR9ewyx"
   },
   "source": [
    "## VGG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RF0r5z--TTQ0"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Vgg(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Vgg, self).__init__()\n",
    "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
    "                     'std': [1, 1, 1],\n",
    "                     'imageSize': [224, 224, 3]}\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        x1 = self.conv1_1(x0)\n",
    "        x2 = self.relu1_1(x1)\n",
    "        x3 = self.conv1_2(x2)\n",
    "        x4 = self.relu1_2(x3)\n",
    "        x5 = self.pool1(x4)\n",
    "        x6 = self.conv2_1(x5)\n",
    "        x7 = self.relu2_1(x6)\n",
    "        x8 = self.conv2_2(x7)\n",
    "        x9 = self.relu2_2(x8)\n",
    "        x10 = self.pool2(x9)\n",
    "        x11 = self.conv3_1(x10)\n",
    "        x12 = self.relu3_1(x11)\n",
    "        x13 = self.conv3_2(x12)\n",
    "        x14 = self.relu3_2(x13)\n",
    "        x15 = self.conv3_3(x14)\n",
    "        x16 = self.relu3_3(x15)\n",
    "        x17 = self.pool3(x16)\n",
    "        x18 = self.conv4_1(x17)\n",
    "        x19 = self.relu4_1(x18)\n",
    "        x20 = self.conv4_2(x19)\n",
    "        x21 = self.relu4_2(x20)\n",
    "        x22 = self.conv4_3(x21)\n",
    "        x23 = self.relu4_3(x22)\n",
    "        x24 = self.pool4(x23)\n",
    "        x25 = self.conv5_1(x24)\n",
    "        x26 = self.relu5_1(x25)\n",
    "        x27 = self.conv5_2(x26)\n",
    "        x28 = self.relu5_2(x27)\n",
    "        x29 = self.conv5_3(x28)\n",
    "        x30 = self.relu5_3(x29)\n",
    "        x31_preflatten = self.pool5(x30)\n",
    "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
    "        x32 = self.fc6(x31)\n",
    "        x33 = self.relu6(x32)\n",
    "        x34 = self.dropout6(x33)\n",
    "        x35 = self.fc7(x34)\n",
    "        x36 = self.relu7(x35)\n",
    "        x37 = self.dropout7(x36)\n",
    "        x38 = self.fc8(x37)\n",
    "        return x38\n",
    "\n",
    "def vgg_face(weights_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    load imported model instance\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): If set, loads model weights from the given path\n",
    "    \"\"\"\n",
    "    model = Vgg()\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6znD8m3e0UX"
   },
   "source": [
    "### Load pretrained weights on vgg_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-mz1UwaITTQ1"
   },
   "outputs": [],
   "source": [
    "vgg = vgg_face(\"vgg_face_dag.pth\")\n",
    "vgg.fc8 = nn.Linear(in_features=4096, out_features=7, bias=True)\n",
    "vgg = vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfE_h1R5e44U"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2Vz0Yf4e7kU"
   },
   "source": [
    "### Initial evaluation on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qyUWv3xVg6j8"
   },
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "def eval_model(net, loader):\n",
    "  net.eval()\n",
    "  acc, loss = 0., 0.\n",
    "  c = 0\n",
    "  for x, y in loader:\n",
    "    with torch.no_grad():\n",
    "      # No need to compute gradient here thus we avoid storing intermediary activations\n",
    "      logits = net(x.to(device)).cpu()\n",
    "\n",
    "    loss += cross_entropy(logits, y).item()\n",
    "    preds = logits.argmax(dim=1)\n",
    "    acc += (preds.numpy() == y.numpy()).sum()\n",
    "    c += len(x)\n",
    "\n",
    "  acc /= c\n",
    "  loss /= len(loader)\n",
    "  net.train()\n",
    "  return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "nzWqyNeOTTQ2",
    "outputId": "8b5bf9d4-a3e9-4e9e-fbf7-ae8c66c5f1b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-ca7593c16a03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minitial_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initial accuracy/loss on val: {round(100 * initial_acc, 2)}/{round(initial_loss, 4)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-a2738320c5a1>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(net, loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;31m# No need to compute gradient here thus we avoid storing intermediary activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "initial_acc, initial_loss = eval_model(vgg, val_loader)\n",
    "print(f\"Initial accuracy/loss on val: {round(100 * initial_acc, 2)}/{round(initial_loss, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEUMsdH-fF79"
   },
   "source": [
    "### Training\n",
    "\n",
    "Training will be performed with adam optimizer, using a base learning rate of 5e-5 and polynomial decay on all epochs with a power of 0.5, for 75 epochs.  \n",
    "We will save the best model according to the accuracy on the validation set.  \n",
    "These parameters follow the experimental protocol of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "HFqWHdsKfC_O"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolynomialLR\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mvgg\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00005\u001b[39m)\n\u001b[1;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m PolynomialLR(optimizer, total_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m75\u001b[39m, power\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      7\u001b[0m nb_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m75\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vgg' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(vgg.parameters(), lr=0.00005)\n",
    "scheduler = PolynomialLR(optimizer, total_iters=75, power=0.5)\n",
    "\n",
    "nb_epochs = 75\n",
    "\n",
    "train_accs, train_losses = [], []\n",
    "val_accs, val_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "27MCDp2JTTQ2",
    "outputId": "6910f8b4-989b-4db6-a4b0-66b44da004d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [03:40<00:00,  3.44batch/s, loss=1.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75, train acc/loss: 48.54/1.4031, val acc/loss: 68.75/0.0096, time 224s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [03:39<00:00,  3.45batch/s, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/75, train acc/loss: 49.32/1.3716, val acc/loss: 56.25/0.0135, time 220s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [03:39<00:00,  3.45batch/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/75, train acc/loss: 49.06/1.3741, val acc/loss: 31.25/0.0164, time 221s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [03:39<00:00,  3.45batch/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/75, train acc/loss: 48.95/1.3735, val acc/loss: 56.25/0.0143, time 220s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [03:39<00:00,  3.45batch/s, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/75, train acc/loss: 49.37/1.3782, val acc/loss: 43.75/0.0203, time 220s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [03:40<00:00,  3.44batch/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/75, train acc/loss: 49.01/1.3786, val acc/loss: 68.75/0.0118, time 221s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [03:40<00:00,  3.43batch/s, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/75, train acc/loss: 49.29/1.3739, val acc/loss: 68.75/0.0101, time 221s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [03:38<00:00,  3.46batch/s, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/75, train acc/loss: 49.36/1.375, val acc/loss: 75.0/0.016, time 221s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/758 [00:05<04:02,  3.05batch/s, loss=1.52]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-01e2e95f0c23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mrunning_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "best_acc = 0\n",
    "for epoch in range(nb_epochs):\n",
    "  with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "    start = time.time()\n",
    "    running_acc, running_loss = 0., 0.\n",
    "    c = 0\n",
    "    for x, y in tepoch:\n",
    "      x, y = x.to(device), y.to(device)\n",
    "\n",
    "      optimizer.zero_grad()  # Clear previous gradients\n",
    "      logits = vgg(x)\n",
    "      loss = cross_entropy(logits, y)\n",
    "      loss.backward()  # Compute gradients\n",
    "      optimizer.step()  # Update weights with gradients\n",
    "      scheduler.step()\n",
    "\n",
    "      running_acc += (logits.argmax(dim=1).cpu().numpy() == y.cpu().numpy()).sum()\n",
    "      running_loss += loss.item()\n",
    "      c += len(x)\n",
    "      tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_acc, train_loss = running_acc / c, running_loss / len(train_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_acc, val_loss = eval_model(vgg, val_loader, cross_entropy)\n",
    "    if val_acc > best_acc:\n",
    "      best_acc = val_acc\n",
    "      torch.save(vgg.state_dict(),\"vgg_best_param.pth\")\n",
    "    val_accs.append(val_acc)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{nb_epochs}, \"\n",
    "        f\"train acc/loss: {round(100 * train_acc, 2)}/{round(train_loss, 4)}, \"\n",
    "        f\"val acc/loss: {round(100 * val_acc, 2)}/{round(val_loss, 4)}, \"\n",
    "        f\"time {int(time.time() - start)}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INtYIpmcosHw"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(list(range(nb_epochs)), train_accs, label=\"Train\")\n",
    "plt.plot(list(range(nb_epochs)), val_accs, label=\"Val\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(list(range(nb_epochs)), train_losses, label=\"Train\")\n",
    "plt.plot(list(range(nb_epochs)), val_losses, label=\"Val\")\n",
    "plt.title(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry9hgujofKMO"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tu_xXdYLR-GQ"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"vgg_best_param.pth\")\n",
    "vgg.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "939OxVuZJsCc"
   },
   "outputs": [],
   "source": [
    "test_acc, test_loss = eval_model(vgg, test_loader, cross_entropy)\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF-ecOeESOBK"
   },
   "source": [
    "# II - Vgg with Pal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train to reproduce the best results of the paper using PAL. Consequently, we will use PAL on the 15th layer, with Grad\\*Input as attribution map and the half mean channel strategy. The other parameters will follow the paper, and thus will not change compared to part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TQmFZkQvSeuT"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"landmark.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./landmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "69CmLPTeSurv"
   },
   "outputs": [],
   "source": [
    "!mkdir landmark/train\n",
    "!mkdir landmark/test\n",
    "!mv landmark/landmark/train_* landmark/train\n",
    "!mv landmark/landmark/test_* landmark/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders\n",
    "The process is the same as in part I, with the added priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ulx3aSxCS_3z"
   },
   "outputs": [],
   "source": [
    "class JoinImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, landmark_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.landmark_dir = landmark_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        land_path = os.path.join(self.landmark_dir, self.img_labels.iloc[idx, 0])\n",
    "        landmark = read_image(land_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, landmark, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P78LHeOuTjm3",
    "outputId": "9d035d48-9cb6-41df-814f-9b00642b266c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nb batches in train: 758\n"
     ]
    }
   ],
   "source": [
    "train_data = JoinImageDataset(\"train_list_label.csv\",\"./aligned/train\", \"./landmark/train\", transform=trans)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4)\n",
    "print(f\"\\nNb batches in train: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Nv1zFNLcT4xW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_data = JoinImageDataset(\"test_list_label.csv\",\"./aligned/test\",\"./landmark/test\", transform=trans)\n",
    "\n",
    "test_indices, val_indices = train_test_split(list(range(len(test_data.img_labels.Label))), test_size=0.5, stratify=test_data.img_labels.Label)\n",
    "\n",
    "val_data = torch.utils.data.Subset(test_data, val_indices)\n",
    "test_data = torch.utils.data.Subset(test_data, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCcVeomlUEx3",
    "outputId": "e46847fe-6d4d-4c97-c9be-4a0125cd141e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nb batches in test: 92\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=True, num_workers=4)\n",
    "print(f\"\\nNb batches in test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SlqNHILrUKMw",
    "outputId": "cae9fb2f-4d78-42ee-ae55-2f2f319d0d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nb batches in val: 92\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=True, num_workers=4)\n",
    "print(f\"\\nNb batches in val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Vgg class\n",
    "The class now outputs input and output of layer 15 in order to apply PAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UOSjJUATUOx4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VggPal(Vgg):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VggPal, self).__init__()\n",
    "  \n",
    "    def forward(self, x0):\n",
    "        x1 = self.conv1_1(x0)\n",
    "        x2 = self.relu1_1(x1)\n",
    "        x3 = self.conv1_2(x2)\n",
    "        x4 = self.relu1_2(x3)\n",
    "        x5 = self.pool1(x4)\n",
    "        x6 = self.conv2_1(x5)\n",
    "        x7 = self.relu2_1(x6)\n",
    "        x8 = self.conv2_2(x7)\n",
    "        x9 = self.relu2_2(x8)\n",
    "        x10 = self.pool2(x9)\n",
    "        x11 = self.conv3_1(x10)\n",
    "        x12 = self.relu3_1(x11)\n",
    "        x13 = self.conv3_2(x12)\n",
    "        x14 = self.relu3_2(x13)\n",
    "        x15 = self.conv3_3(x14)\n",
    "        x16 = self.relu3_3(x15)\n",
    "        x17 = self.pool3(x16)\n",
    "        x18 = self.conv4_1(x17)\n",
    "        x19 = self.relu4_1(x18)\n",
    "        x20 = self.conv4_2(x19)\n",
    "        x21 = self.relu4_2(x20)\n",
    "        x22 = self.conv4_3(x21)\n",
    "        x23 = self.relu4_3(x22)\n",
    "        x24 = self.pool4(x23)\n",
    "        x25 = self.conv5_1(x24)\n",
    "        x26 = self.relu5_1(x25)\n",
    "        x27 = self.conv5_2(x26)\n",
    "        x28 = self.relu5_2(x27)\n",
    "        x29 = self.conv5_3(x28)\n",
    "        x30 = self.relu5_3(x29)\n",
    "        x31_preflatten = self.pool5(x30)\n",
    "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
    "        x32 = self.fc6(x31)\n",
    "        x33 = self.relu6(x32)\n",
    "        x34 = self.dropout6(x33)\n",
    "        x35 = self.fc7(x34)\n",
    "        x36 = self.relu7(x35)\n",
    "        x37 = self.dropout7(x36)\n",
    "        x38 = self.fc8(x37)\n",
    "        \n",
    "        return x24,x25,x38\n",
    "\n",
    "def vggpal_face(weights_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    load imported model instance\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): If set, loads model weights from the given path\n",
    "    \"\"\"\n",
    "    model = VggPal()\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights on vgg_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "pJKrhFKVVgQM"
   },
   "outputs": [],
   "source": [
    "vggpal = vggpal_face(\"vgg_face_dag.pth\")\n",
    "vggpal.fc8 = nn.Linear(in_features=4096, out_features=7, bias=True)\n",
    "vggpal = vggpal.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of PAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "gjeV_zDwZHx8"
   },
   "outputs": [],
   "source": [
    "def Grad(inputs, outputs) :\n",
    "    outputs_sum = outputs.sum()\n",
    "    inputs.retain_grad()\n",
    "    outputs.retain_grad()\n",
    "    outputs_sum.backward(retain_graph=True)\n",
    "    return torch.abs(inputs.grad)\n",
    "\n",
    "def GradxInput(inputs, outputs) :\n",
    "    Grad_val = Grad(inputs, outputs)\n",
    "    return Grad_val * inputs\n",
    "\n",
    "def PAL(inputs, outputs, prior, attribution_method, channel_strategy=None) :\n",
    "    attribution_map = attribution_method(inputs, outputs)\n",
    "    \n",
    "    if channel_strategy == \"half_mean\" :\n",
    "        nb_class = attribution_map.shape[1]\n",
    "        attribution_map[:, int(nb_class/2):, :, :]\n",
    "    \n",
    "    if channel_strategy == \"half_mean\" or channel_strategy == \"mean\" :\n",
    "        attribution_map = attribution_map.mean(1).unsqueeze(1)\n",
    "    \n",
    "    attribution_map_resize = transforms.Resize(attribution_map.shape[-2:])\n",
    "    \n",
    "    prior = attribution_map_resize(prior)\n",
    "    \n",
    "    std = attribution_map.view(attribution_map.size(0), -1).std(1)\n",
    "    mean = attribution_map.view(attribution_map.size(0), -1).mean(1)\n",
    "    \n",
    "    \n",
    "    res = (attribution_map - mean.view(-1, 1, 1, 1)) / std.view(-1, 1, 1, 1)\n",
    "    res = res * prior.unsqueeze(1)\n",
    "    \n",
    "    res = res.view(res.size(0), -1).sum(1)\n",
    "    res = -res\n",
    "    return res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "puU0EGIdZOjx"
   },
   "outputs": [],
   "source": [
    "def tot_loss(li, lo, prior, logits, y):\n",
    "    pa_loss = PAL(li, lo, prior, GradxInput, \"half_mean\")\n",
    "    ce_loss = cross_entropy(logits, y)\n",
    "    return pa_loss + ce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial evaluation on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "FOqWGhSqZBDc"
   },
   "outputs": [],
   "source": [
    "def eval_modelpal(net, loader):\n",
    "  net.eval()\n",
    "  acc, loss = 0., 0.\n",
    "  c = 0\n",
    "  for x, prior, y in loader:\n",
    "    li, lo, logits = net(x.to(device))\n",
    "    li, lo, logits = li.cpu(), lo.cpu(), logits.cpu()\n",
    "    loss += tot_loss(li, lo, prior, logits, y).item()\n",
    "    preds = logits.argmax(dim=1)\n",
    "    acc += (preds.numpy() == y.numpy()).sum()\n",
    "    c += len(x)\n",
    "\n",
    "  acc /= c\n",
    "  loss /= len(loader)\n",
    "  net.train()\n",
    "  return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "UYNlujThVo6i",
    "outputId": "5ceda1aa-248c-4b0b-fd4d-094fe01d9fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n",
      "512\n",
      "512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m initial_acc, initial_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_modelpal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvggpal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial accuracy/loss on val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m initial_acc, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(initial_loss, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [45], line 8\u001b[0m, in \u001b[0;36meval_modelpal\u001b[0;34m(net, loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m li, lo, logits \u001b[38;5;241m=\u001b[39m net(x\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      7\u001b[0m li, lo, logits \u001b[38;5;241m=\u001b[39m li\u001b[38;5;241m.\u001b[39mcpu(), lo\u001b[38;5;241m.\u001b[39mcpu(), logits\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtot_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      9\u001b[0m preds \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39msum()\n",
      "Cell \u001b[0;32mIn [42], line 2\u001b[0m, in \u001b[0;36mtot_loss\u001b[0;34m(li, lo, prior, logits, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtot_loss\u001b[39m(li, lo, prior, logits, y):\n\u001b[0;32m----> 2\u001b[0m   pa_loss \u001b[38;5;241m=\u001b[39m \u001b[43mpal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mli\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43mGradxInput\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhalf_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m   ce_loss \u001b[38;5;241m=\u001b[39m cross_entropy(logits, y)\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m pa_loss \u001b[38;5;241m+\u001b[39m ce_loss\n",
      "Cell \u001b[0;32mIn [41], line 13\u001b[0m, in \u001b[0;36mpal\u001b[0;34m(inputs, outputs, prior, attribution_method, channel_strategy)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpal\u001b[39m(inputs, outputs, prior, attribution_method, channel_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) :\n\u001b[0;32m---> 13\u001b[0m     attribution_map \u001b[38;5;241m=\u001b[39m \u001b[43mattribution_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m channel_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhalf_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m :\n\u001b[1;32m     16\u001b[0m         nb_class \u001b[38;5;241m=\u001b[39m attribution_map\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn [41], line 9\u001b[0m, in \u001b[0;36mGradxInput\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGradxInput\u001b[39m(inputs, outputs) :\n\u001b[0;32m----> 9\u001b[0m     Grad_val \u001b[38;5;241m=\u001b[39m \u001b[43mGrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Grad_val \u001b[38;5;241m*\u001b[39m inputs\n",
      "Cell \u001b[0;32mIn [41], line 5\u001b[0m, in \u001b[0;36mGrad\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs\u001b[38;5;241m.\u001b[39mretain_grad()\n\u001b[1;32m      4\u001b[0m outputs\u001b[38;5;241m.\u001b[39mretain_grad()\n\u001b[0;32m----> 5\u001b[0m \u001b[43moutputs_sum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mabs(inputs\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_acc, initial_loss = eval_modelpal(vggpal, val_loader)\n",
    "print(f\"Initial accuracy/loss on val: {round(100 * initial_acc, 2)}/{round(initial_loss, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "itbzcRpwV9Lb"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vggpal.parameters(), lr=0.00005)\n",
    "scheduler = PolynomialLR(optimizer, total_iters=75, power=0.5)\n",
    "\n",
    "nb_epochs = 75\n",
    "\n",
    "train_accs, train_losses = [], []\n",
    "val_accs, val_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "X5f7XGWyWGZ1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/758 [00:16<3:31:05, 16.73s/batch, loss=5.18e+4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [54], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[1;32m     12\u001b[0m li, lo ,logits \u001b[38;5;241m=\u001b[39m vggpal(x)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtot_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights with gradients\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [49], line 2\u001b[0m, in \u001b[0;36mtot_loss\u001b[0;34m(li, lo, prior, logits, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtot_loss\u001b[39m(li, lo, prior, logits, y):\n\u001b[0;32m----> 2\u001b[0m     pa_loss \u001b[38;5;241m=\u001b[39m \u001b[43mPAL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGradxInput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhalf_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     ce_loss \u001b[38;5;241m=\u001b[39m cross_entropy(logits, y)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa_loss \u001b[38;5;241m+\u001b[39m ce_loss\n",
      "Cell \u001b[0;32mIn [48], line 13\u001b[0m, in \u001b[0;36mPAL\u001b[0;34m(inputs, outputs, prior, attribution_method, channel_strategy)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPAL\u001b[39m(inputs, outputs, prior, attribution_method, channel_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) :\n\u001b[0;32m---> 13\u001b[0m     attribution_map \u001b[38;5;241m=\u001b[39m \u001b[43mattribution_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m channel_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhalf_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m :\n\u001b[1;32m     16\u001b[0m         nb_class \u001b[38;5;241m=\u001b[39m attribution_map\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn [48], line 9\u001b[0m, in \u001b[0;36mGradxInput\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGradxInput\u001b[39m(inputs, outputs) :\n\u001b[0;32m----> 9\u001b[0m     Grad_val \u001b[38;5;241m=\u001b[39m \u001b[43mGrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Grad_val \u001b[38;5;241m*\u001b[39m inputs\n",
      "Cell \u001b[0;32mIn [48], line 5\u001b[0m, in \u001b[0;36mGrad\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs\u001b[38;5;241m.\u001b[39mretain_grad()\n\u001b[1;32m      4\u001b[0m outputs\u001b[38;5;241m.\u001b[39mretain_grad()\n\u001b[0;32m----> 5\u001b[0m \u001b[43moutputs_sum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mabs(inputs\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "best_acc = 0\n",
    "for epoch in range(nb_epochs):\n",
    "  with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "    start = time.time()\n",
    "    running_acc, running_loss = 0., 0.\n",
    "    c = 0\n",
    "    for x, prior, y in tepoch:\n",
    "      x, prior, y = x.to(device), prior.to(device),y.to(device)\n",
    "\n",
    "      optimizer.zero_grad()  # Clear previous gradients\n",
    "      li, lo ,logits = vggpal(x)\n",
    "      loss = tot_loss(li, lo, prior, logits, y)\n",
    "      loss.backward()  # Compute gradients\n",
    "      optimizer.step()  # Update weights with gradients\n",
    "      scheduler.step()\n",
    "\n",
    "      running_acc += (logits.argmax(dim=1).cpu().numpy() == y.cpu().numpy()).sum()\n",
    "      running_loss += loss.item()\n",
    "      c += len(x)\n",
    "      tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_acc, train_loss = running_acc / c, running_loss / len(train_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_acc, val_loss = eval_model(vggpal, val_loader, cross_entropy)\n",
    "    if val_acc > best_acc:\n",
    "      best_acc = val_acc\n",
    "      torch.save(vggpal.state_dict(),\"vggpal_best_param.pth\")\n",
    "    val_accs.append(val_acc)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{nb_epochs}, \"\n",
    "        f\"train acc/loss: {round(100 * train_acc, 2)}/{round(train_loss, 4)}, \"\n",
    "        f\"val acc/loss: {round(100 * val_acc, 2)}/{round(val_loss, 4)}, \"\n",
    "        f\"time {int(time.time() - start)}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3DdRcYgXx5H"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(list(range(nb_epochs)), train_accs, label=\"Train\")\n",
    "plt.plot(list(range(nb_epochs)), val_accs, label=\"Val\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(list(range(nb_epochs)), train_losses, label=\"Train\")\n",
    "plt.plot(list(range(nb_epochs)), val_losses, label=\"Val\")\n",
    "plt.title(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdM_4qN_X0T3"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"vggpal_best_param.pth\")\n",
    "vgg.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OwF7dnQX4Lt"
   },
   "outputs": [],
   "source": [
    "test_acc, test_loss = eval_model(vgg, test_loader, cross_entropy)\n",
    "test_acc, test_loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
